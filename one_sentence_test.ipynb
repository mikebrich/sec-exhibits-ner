{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57fb0d1-42c5-48ed-9c70-5682ac43baad",
   "metadata": {},
   "source": [
    "## Prereqs\n",
    "\n",
    "1. You have jupyterlab running in your base conda environment. \n",
    "1. Create the `nlp_stanza` environment via the readme's instructions. \n",
    "1. Add that environment's python kernel to jupyterlab via the following code. This will also download the `spacy` model. \n",
    "    ```\n",
    "    $ conda activate nlp_stanza          \n",
    "    (nlp_stanza)$ conda install ipykernel\n",
    "    (nlp_stanza)$ python -m ipykernel install --user --name=nlp_stanza --display-name \"NLP Stanza Env\"\n",
    "    (nlp_stanza)$ python -m spacy download en_core_web_sm\n",
    "    (nlp_stanza)$ conda deactivate\n",
    "    $ jupyter lab          \n",
    "    ```\n",
    "1. Make sure this script is being run with the nlp_stanza conda environment's kernel. In the upper right corner, click the kernel name and change to \"NLP Stanza Env\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe85c0a-f930-4ce6-95fc-971780e48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in terminaL\n",
    "\n",
    "# > conda activate nlp_stanza\n",
    "# > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c4ccb8-38df-4746-9603-697af84bfe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 14:28:08 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-30 14:28:08 INFO: Using device: cpu\n",
      "2023-03-30 14:28:08 INFO: Loading: tokenize\n",
      "2023-03-30 14:28:08 INFO: Loading: pos\n",
      "2023-03-30 14:28:08 INFO: Loading: lemma\n",
      "2023-03-30 14:28:09 INFO: Loading: constituency\n",
      "2023-03-30 14:28:10 INFO: Loading: depparse\n",
      "2023-03-30 14:28:10 INFO: Loading: sentiment\n",
      "2023-03-30 14:28:11 INFO: Loading: ner\n",
      "2023-03-30 14:28:13 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DonsLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DonsLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DonsLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DonsLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\DonsLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download('en') # This downloads the English models for the neural pipelin\n",
    "stanza_nlp = stanza.Pipeline('en', download_method=None) # This sets up a default neural pipeline in English\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\",)  \n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce0b5c-d622-4f76-b30d-004cf5dd0f82",
   "metadata": {},
   "source": [
    "### One sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5470702f-babd-4408-81aa-65fac786b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Seller produces hot stamping foil which conforms and meets the Specification Requirements submitted, accepted and in Seller's possession for the Specification numbers listed attached in the Exhibit A., hereafter referred nto as 'Products'.\"\n",
    "\n",
    "# doc = stanza_nlp(sentence)\n",
    "\n",
    "# subj_verb_linkages = []\n",
    "\n",
    "# for word in doc.sentences[0].words:    # for each word in the sentence\n",
    "#     if word.deprel == 'root':          # if a word is the root, its dependency relation label is 'root'. thus if this is true, the curr word = root word\n",
    "        \n",
    "#         root_verb = word.text          # save the root verb  \n",
    "#         root_id = word.id              # get the words id \n",
    "        \n",
    "#         for w in doc.sentences[0].words:                      # loop over words in the sentence\n",
    "#             if w.head == root_id and w.deprel == 'nsubj':     # if words head attribute = root_id, then its a a direct dependent of the root (is a child of the root)\n",
    "                \n",
    "#                 subject = w.text                              # save the associated subject\n",
    "        \n",
    "\n",
    "# subj_verb_linkages = [subject, root_verb]\n",
    "\n",
    "# print(f\"Root Verb: {root_verb}, Subject: {subject}\")\n",
    "\n",
    "# subj_verb_linkages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985fce8-fceb-4ddf-89f1-202452b451db",
   "metadata": {},
   "source": [
    "### New ner() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b6ea97-0878-4b75-a51a-3d209b748a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(sentence): \n",
    "    \n",
    "    entities = []\n",
    "    verbs = []\n",
    "    subjects = []\n",
    "    subj_verb_linkages = []\n",
    "    \n",
    "    #Find the entities in the sentence\n",
    "    words  = nltk.word_tokenize(sentence)        # break down the sentence into words\n",
    "    tagged = nltk.pos_tag(words)                 # tag the words with Part of Speech \n",
    "    chunks = nltk.ne_chunk(tagged, binary=False) # binary = False named entities are classified (i.e PERSON, ORGANIZATION)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):              # hasattr(obj, key) -- checking if chunks have a label or not \n",
    "            entities.append(' '.join(c[0] for c in chunk)) # append entities to array\n",
    "    \n",
    "    \n",
    "    #Find the verbs/subjects in the sentence\n",
    "             # load in the spacy model\n",
    "    doc = spacy_nlp(sentence)                          # create spacy doc object\n",
    "    \n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]     # traverse thru the tokens, find the verbs\n",
    "    subjects = [token.text for token in doc if token.dep_ == \"nsubj\"]  # traverse thru the tokens, find the subjects\n",
    "    \n",
    "    \n",
    "    #Find the Root Subject-verb linkages in the sentences using stanza\n",
    "    doc = stanza_nlp(sentence)\n",
    "    \n",
    "    subject, root_verb = None, None\n",
    "    \n",
    "    for word in doc.sentences[0].words:    # for each word in the sentence\n",
    "        if word.deprel == 'root':          # if a word is the root, its dependency relation label is 'root'. thus if this is true, the curr word = root word\n",
    "\n",
    "            root_verb = word.text          # save the root verb  \n",
    "            root_id = word.id              # get the words id \n",
    "            \n",
    "            for w in doc.sentences[0].words:                      # loop over words in the sentence\n",
    "                if w.head == root_id and w.deprel == 'nsubj':     # if words head attribute = root_id, then its a a direct dependent of the root (is a child of the root)\n",
    "                    subject = w.text \n",
    "    \n",
    "    if subject and root_verb: # not empty\n",
    "        subj_verb_linkages = [subject, root_verb]   # subj_verb linkages array \n",
    "    \n",
    "        \n",
    "    return {'entities':entities, \n",
    "            'verbs':verbs,\n",
    "            'subjects':subjects,\n",
    "            'subj_verb_linkages':subj_verb_linkages} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e7e38e3-e36b-470d-ae61-08dfae9a7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Seller produces hot stamping foil which conforms and meets the Specification Requirements submitted, accepted and in Seller's possession for the Specification numbers listed attached in the Exhibit A., hereafter referred nto as 'Products'.\"\n",
    "\n",
    "\n",
    "temp = ner('Buyer agrees to purchase foil requirements for current users, which are wholly owned subsidiaries of Baxter Healthcare Corporation.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0593aedd-f0eb-47e7-9912-a87c12dea282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['Buyer', 'Baxter Healthcare Corporation'],\n",
       " 'verbs': ['agrees', 'purchase', 'owned'],\n",
       " 'subjects': ['Buyer', 'which'],\n",
       " 'subj_verb_linkages': ['Buyer', 'agrees']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abad7e-7e11-4fe3-9378-e10660ff9c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Stanza Env",
   "language": "python",
   "name": "nlp_stanza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
